# Venue Website Scraper Workflow
# French Wedding Style — Hermetic Source of Truth
#
# This is the SINGLE source of truth for the venue scraping workflow.
# Source of Truth Hierarchy:
#   1. workflow.yml (this file) — logic, API config, execution phases
#   2. instructions.md — error handling, content cleaning, safety rules
#
# Credentials are loaded from: ../../.env

name: scrape-venue-site
description: >
  Scrapes venue websites from Airtable using Firecrawl, converts them to
  clean Markdown, and writes the content back to the venue_url_scraped
  field in Airtable. Processes in batches of 5 with user confirmation
  between batches. Claude orchestrates directly via curl.
root_path: ./

# ─── Configuration ───────────────────────────────────────────────
config:
  communication_language: en
  batch_size: 5
  max_content_length: 95000        # Airtable long text field character limit
  consecutive_failure_threshold: 2  # Pause and ask user after N consecutive failures
  rate_limit_delay: 2               # Seconds between Firecrawl API calls
  manual_check_marker: "MANUAL_CHECK"

# ─── Credentials ─────────────────────────────────────────────────
credentials:
  env_file: ../../.env
  required_keys:
    - AIRTABLE_API_KEY
    - AIRTABLE_BASE_ID
    - FIRECRAWL_API_KEY

# ─── Source Columns ─────────────────────────────────────────────
# venue_url is MANDATORY — if empty, skip the record entirely.
# The 3 listing-site columns are OPTIONAL — scrape only if populated.
# All scraped content is combined into venue_url_scraped with bold
# headings to label each source.
source_columns:
  primary:
    field: venue_url
    label: "**Venue Website**"
    required: true               # Record is skipped if this is empty
  listing_sites:
    - field: chateaubee_url
      label: "**ChateauBee**"
      required: false            # Scrape only if cell has a URL
    - field: wedinspire_url
      label: "**WedInspire**"
      required: false
    - field: fwv_url
      label: "**French Wedding Venues**"
      required: false

# ─── API Configuration ──────────────────────────────────────────
apis:
  firecrawl:
    auth_header: "Authorization: Bearer {FIRECRAWL_API_KEY}"
    content_type: "application/json"

    # Step 1: Discover all pages on the venue domain
    map:
      endpoint: "https://api.firecrawl.dev/v2/map"
      method: POST
      request_body:
        url: "{source_url}"
      response_path: "links"
      success_check: "success == true"

    # Step 2: Scrape each discovered page individually
    # NOTE: onlyMainContent is FALSE — JS-heavy sites lose actual content with true.
    # Extra nav/footer noise is handled by clean_markdown() in the script.
    scrape:
      endpoint: "https://api.firecrawl.dev/v2/scrape"
      method: POST
      request_body:
        url: "{page_url}"
        formats: ["markdown"]
        onlyMainContent: false
        timeout: 120000
        waitFor: 8000
      response_path: "data.markdown"
      success_check: "success == true"

    # Page filtering rules (applied to map results)
    # NOTE: Map + multi-page scrape applies ONLY to venue_url (primary).
    # Listing-site URLs are single-page scrapes (no map step needed).
    page_filter:
      max_pages: 20                    # Cap pages per venue to control credits
      exclude_patterns:
        - "sitemap.xml"
        - "/blog/"
        - "/news/"
        - "/press"
        - "/presse"
        - "/tag/"
        - "/category/"
        - "/author/"
        - "/cart"
        - "/checkout"
        - "/login"
        - "/account"
        - "/privacy"
        - "/terms"
        - "/legal"
        - "/cookie"
      # When both French and English pages exist, prefer English
      prefer_english: false
      # Always include the venue_url itself as the first page
      always_include_source: true

  airtable:
    base_url: "https://api.airtable.com/v0"
    auth_header: "Authorization: Bearer {AIRTABLE_API_KEY}"
    content_type: "application/json"

    list_records:
      method: GET
      url: "{base_url}/{AIRTABLE_BASE_ID}/Venues"
      params:
        # Eligible = venue_url is populated AND target field is empty
        filterByFormula: "AND(NOT({venue_url}=''),{venue_url_scraped}='')"
        fields: ["venue_url", "venue_address", "gps_coordinates", "chateaubee_url", "wedinspire_url", "fwv_url"]
        pageSize: 100

    update_record:
      method: PATCH
      url: "{base_url}/{AIRTABLE_BASE_ID}/Venues/{record_id}"
      body:
        fields:
          venue_url_scraped: "{combined_markdown}"

    update_gps:
      method: PATCH
      url: "{base_url}/{AIRTABLE_BASE_ID}/Venues/{record_id}"
      body:
        fields:
          gps_coordinates: "{lat}, {lon}"

# ─── Airtable Schema ────────────────────────────────────────────
schema:
  table_name: Venues
  source_fields:
    primary: venue_url                 # Read-only — never modify. MUST have a URL.
    address: venue_address             # Read-only — used for geocoding.
    listing_sites:                     # Read-only — never modify. Scraped only if populated.
      - chateaubee_url
      - wedinspire_url
      - fwv_url
  targets:
    scraped_content: venue_url_scraped # Write target — only write if empty
    gps: gps_coordinates               # Write target — "lat, lon" string from geocoding
    full_json: full_venue_json         # Write target — full venue JSON record (~320 fields)
    summary_json: summary_venue_json   # Write target — reduced venue JSON record (20 core fields)

# ─── File References ─────────────────────────────────────────────
files:
  instructions: instructions/instructions.md
  script: scripts/process_venue.py       # Unified orchestrator — handles scrape/clean/categorize/write
  working_log: working/workings_temp.md
  payload_temp: working/payload.json
  output_dir: output/

# ─── Execution ──────────────────────────────────────────────────
# Two-stage pipeline:
#   Stage 1 (script): process_venue.py --scrape-only handles map/scrape/basic-clean
#                      and saves raw files to working/
#   Stage 2 (Claude): reads raw files, strips remaining noise semantically,
#                      organizes into structured sections, writes to Airtable via MCP
execution:
  read_order:
    - ../../.env

  phases:
    - name: "Phase A: Discovery"
      steps:
        - "Read .env from workspace root — extract API credentials"
        - "curl Airtable list endpoint with filterByFormula to find eligible records"
        - "Handle pagination: if response contains 'offset', fetch next page"
        - "Build array of {record_id, venue_url, venue_address, gps_coordinates, chateaubee_url, wedinspire_url, fwv_url} tuples"
        - "Report total eligible records to user (and how many have listing-site URLs, how many need geocoding)"
        - "Create/append session header to working/workings_temp.md"

    - name: "Phase B: Process Venues (Each in Own Task Agent)"
      # CONTEXT ISOLATION: Each venue's full pipeline (scrape → structure → write)
      # runs in its own Task agent. The orchestrator never touches raw content —
      # it only sees one-line status summaries. This prevents memory/context
      # accumulation that caused STATUS_STACK_BUFFER_OVERRUN on Windows.
      steps:
        - "Select next batch_size records from eligible list"
        - "For each record, launch a Task agent (subagent_type: general-purpose)"
        - "  Process ONE venue at a time (sequentially) to keep memory low"
        - "  Agent handles entire pipeline: scrape → structure → write to Airtable"
        - "  Agent runs: python scripts/process_venue.py --scrape-only ..."
        - "  Agent reads raw files, strips noise, organizes into 5 sections"
        - "  Agent saves structured content to working/structured_{record_id}.md"
        - "  Agent runs: python scripts/process_venue.py --write-file ..."
        - "  Agent returns ONLY: DONE|{venue_url}|{chars} or MANUAL_CHECK|{venue_url}|{reason} or ERROR|{venue_url}|{reason}"
        - "Orchestrator parses one-line result and logs to working/workings_temp.md"
        - "Track consecutive failures — if >= threshold, AskUserQuestion"
        - "After all venues in batch, geocode each record that has venue_address but no gps_coordinates:"
        - '  python scripts/process_venue.py --geocode "{record_id}" "{venue_address}" "{AIRTABLE_KEY}" "{BASE_ID}"'
        - "  Script calls OpenStreetMap Nominatim (free, no key), writes 'lat, lon' to gps_coordinates field"
        - "  Script stdout: GEOCODED|lat,lon  or  GEOCODE_FAIL|reason  or  GEOCODE_SKIP|no address"
        - "  Wait 1.1s between geocode calls (Nominatim rate limit: 1 req/sec)"

    - name: "Phase C: Reporting"
      steps:
        - "Summarize batch: successful, failed, manual_check counts, geocoded/skipped/failed counts"
        - "Save batch report to output/batch-report-{timestamp}.md"
        - "Display summary to user"
        - "Ask user: proceed with next batch of 5?"

standalone: true
